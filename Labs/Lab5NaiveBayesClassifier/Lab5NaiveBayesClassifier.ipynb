{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import *\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Pre-processing for string for standardisation\n",
    "def preprocess_string(str_arg):\n",
    " cleaned_str=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE) \n",
    " cleaned_str=re.sub('(\\s+)',' ',cleaned_str) \n",
    " cleaned_str=cleaned_str.lower() \n",
    " return cleaned_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "class NaiveBayes:\n",
    "    \n",
    "    #A fucntion to pass in unique number of classes of the training data\n",
    "    def __init__(self,unique_classes):\n",
    "        self.classes=unique_classes\n",
    "    \n",
    "    \n",
    "    #A function requried for Naive Bayes class that sees a \"space\" as a\n",
    "    #tokeniser and then uses it to add to a word, and checks back with the\n",
    "    #corresponding dictionary. \n",
    "    def addToBow(self,example,dict_index):\n",
    "        if isinstance(example,np.ndarray): example=example[0]\n",
    "        for token_word in example.split():\n",
    "            self.bow_dicts[dict_index][token_word]+=1\n",
    "          \n",
    "        \n",
    "    #A function required for Naive Bayes class that trains the model, and in this\n",
    "    #with the \"def addToBow\" function, to compute for each category or class\n",
    "    def train(self,dataset,labels):    \n",
    "        self.examples=dataset\n",
    "        self.labels=labels\n",
    "        self.bow_dicts=np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])])\n",
    "        \n",
    "        if not isinstance(self.examples,np.ndarray): self.examples=np.array(self.examples)\n",
    "        if not isinstance(self.labels,np.ndarray): self.labels=np.array(self.labels)\n",
    "            \n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "          \n",
    "            all_cat_examples=self.examples[self.labels==cat] #filter all examples of category == cat\n",
    "            \n",
    "            #For pre-processed examples\n",
    "            cleaned_examples=[preprocess_string(cat_example) for cat_example in all_cat_examples]\n",
    "            cleaned_examples=pd.DataFrame(data=cleaned_examples)\n",
    "            \n",
    "            np.apply_along_axis(self.addToBow,1,cleaned_examples,cat_index)\n",
    "\n",
    "        prob_classes=np.empty(self.classes.shape[0])\n",
    "        all_words=[]\n",
    "        cat_word_counts=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "           \n",
    "            prob_classes[cat_index]=np.sum(self.labels==cat)/float(self.labels.shape[0]) \n",
    "            \n",
    "            count=list(self.bow_dicts[cat_index].values())\n",
    "            cat_word_counts[cat_index]=np.sum(np.array(list(self.bow_dicts[cat_index].values())))+1\n",
    "            \n",
    "            #To call all the words in the dictionary                           \n",
    "            all_words+=self.bow_dicts[cat_index].keys()\n",
    "                                                     \n",
    "        \n",
    "        #To make a unqiue vocab list\n",
    "        self.vocab=np.unique(np.array(all_words))\n",
    "        self.vocab_length=self.vocab.shape[0]\n",
    "                                  \n",
    "        #To calculate the Naive Bayes denominator value                                    \n",
    "        denoms=np.array([cat_word_counts[cat_index]+self.vocab_length+1 for cat_index,cat in enumerate(self.classes)])                                                                          \n",
    "            \n",
    "        #To put into a tuple\n",
    "        self.cats_info=[(self.bow_dicts[cat_index],prob_classes[cat_index],denoms[cat_index]) for cat_index,cat in enumerate(self.classes)]                               \n",
    "        self.cats_info=np.array(self.cats_info)                                 \n",
    "                                              \n",
    "    \n",
    "    #A function required of a Naive Bayes class to estimate the posterior proability of the test vocab words. \n",
    "    def getExampleProb(self,test_example):                                                            \n",
    "        likelihood_prob=np.zeros(self.classes.shape[0])\n",
    "        \n",
    "        for cat_index,cat in enumerate(self.classes): \n",
    "                             \n",
    "            for test_token in test_example.split():                         \n",
    "                test_token_counts=self.cats_info[cat_index][0].get(test_token,0)+1                             \n",
    "                test_token_prob=test_token_counts/float(self.cats_info[cat_index][2])                              \n",
    "                #To prevent underflow problem as mentioned in lecture\n",
    "                likelihood_prob[cat_index]+=np.log(test_token_prob)\n",
    "                                              \n",
    "        post_prob=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            post_prob[cat_index]=likelihood_prob[cat_index]+np.log(self.cats_info[cat_index][1])                                  \n",
    "      \n",
    "        return post_prob\n",
    "    \n",
    "   #A function required of Naive Bayes class to calculate the proability of each test sample\n",
    "   #to all classes and to predict it against the class proability at max. \n",
    "    def test(self,test_set):\n",
    "        \n",
    "        #An empty list to store the prediction of each test \n",
    "        predictions=[]\n",
    "        for example in test_set: \n",
    "                                                                               \n",
    "            cleaned_example=preprocess_string(example) \n",
    "                                            \n",
    "            post_prob=self.getExampleProb(cleaned_example)\n",
    "            \n",
    "            predictions.append(self.classes[np.argmax(post_prob)])\n",
    "                \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wordList</th>\n",
       "      <th>classList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codeine,15mg,for,203,visa,only,codeine,methylm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>peter,with,jose,out,town,you,want,meet,once,wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hydrocodone,vicodin,brand,watson,vicodin,750,1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yay,you,both,doing,fine,working,mba,design,str...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you,have,everything,gain,incredib1e,gains,leng...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            wordList  classList\n",
       "0  codeine,15mg,for,203,visa,only,codeine,methylm...          1\n",
       "1  peter,with,jose,out,town,you,want,meet,once,wh...          0\n",
       "2  hydrocodone,vicodin,brand,watson,vicodin,750,1...          1\n",
       "3  yay,you,both,doing,fine,working,mba,design,str...          0\n",
       "4  you,have,everything,gain,incredib1e,gains,leng...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To import wordsList and classList as a CSV file (previously it was just a \"data\" type file)\n",
    "wordsList = pd.read_csv('wordsList', sep=\" \", header=None)\n",
    "wordsList.columns = ['wordList']\n",
    "classList = pd.read_csv('classList', sep=\" \", header=None)\n",
    "classList.columns = ['classList']\n",
    "wordsList['classList'] = classList['classList']\n",
    "print(wordsList.shape[0])\n",
    "\n",
    "#To print the first 5 rows of wordsList\n",
    "wordsList.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training no of rows X: 66\n",
      "Testing no of rows X: 6\n",
      "Training no of rows y: 66\n",
      "Testing no of rows y: 6\n"
     ]
    }
   ],
   "source": [
    "#Stratified sampling as requested from the question\n",
    "X = wordsList.wordList\n",
    "y = wordsList.classList\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.08)\n",
    "print(\"Training no of rows X: \"+str(len(X_train)))\n",
    "print(\"Testing no of rows X: \"+str(len(X_test)))\n",
    "print(\"Training no of rows y: \"+str(len(y_train)))\n",
    "print(\"Testing no of rows y: \"+str(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training of Model using the Naive Bayes class coded earlier on\n",
    "nb=NaiveBayes(np.unique(y_train)) \n",
    "nb.train(X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Test Set examples:  6\n",
      "Accurarcy for Test Set:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "#To get the prediction results from the test set\n",
    "pclasses=nb.test(X_test) \n",
    "\n",
    "#To check the accurarcy of the model. \n",
    "test_acc=np.sum(pclasses==y_test)/float(y_test.shape[0])\n",
    "print (\"Total number of Test Set examples: \",y_test.shape[0])\n",
    "print (\"Accurarcy for Test Set: \",test_acc*100,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
